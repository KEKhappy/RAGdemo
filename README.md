# demoRAG - Локальная RAG-система с LLM

Проект реализует Retrieval-Augmented Generation (RAG) с использованием локально развернутой языковой модели (LLM) и векторной базы данных для семантического поиска.

## Особенности
- Полностью локальное решение (не требует API-ключей)
- Векторное хранилище на базе ChromaDB
- Поддержка формата PDF
- Поддержка запуска на CPU

### Предварительные требования
- Python 3.9+
- Git
- 16+ ГБ ОЗУ
- Установленные компоненты для llama-cpp:
	- Desktop development with C++
	- Windows 10/11 SDK
	- MSVC v143
	- CMake

## Установка
- Клонируйте репозиторий на свою машину.
- Создайте и активируйте виртуальное окружение:

	python -m venv .venv
	.venv\Scripts\activate.bat
	
- Установите зависимости:

	pip install -r requirements.txt
	
- Создайте аккаунт на https://huggingface.co/
- Получите токен в https://huggingface.co/settings/tokens
- Введите в терминале и войдите при помощи своего токена:

	huggingface-cli login
	

## Запуск
- Сначала нужно запустить обработчик документов:

	py fill_db.py

- При желании можно добавить свои документы в папку data и пересобрать БД:

	py fill_db.py --reset
	
- После этого запускаем query.py с запросом:

	py query.py "{ВАШ ЗАПРОС}"
	

## Тестовые примеры

	py query.py "В каких областях применяются ИК датчики?"
	py query.py "Сколько степеней защиты есть у ИК датчиков?"
	py query.py "Что такое ИК датчик?" 

